# Superhuman Hearing System: Technical Analysis and Architecture

This document presents the technical analysis and proposed architecture for an artificial hearing system with capabilities that approach and in some aspects surpass human audition. The system employs six digital **MEMS** microphones organized in two blocks of three elements each, enabling three-dimensional spatial localization of sound sources with extended dynamic range and advanced real-time digital signal processing capabilities.

The central objective is to create a device capable of identifying the origin of sound sources with angular resolution of three to five degrees, recording high-quality audio across the full audible spectrum, and detecting sporadic acoustic events such as explosions and aircraft with sub-20 millisecond latency. The architecture adopts an all-digital approach using [**I**nter-**I**C **S**ound, **I²S** ou **P**ulse **D**ensity **M**odulation, **PDM** MEMS](https://www.sameskydevices.com/blog/pdm-vs-i2s-comparing-digital-interfaces-in-mems-microphones?srsltid=AfmBOorL6m9D9GzbkECx26VwfIj0ALQLnurtXjhKOByXlG0fRxHb6ecI) microphones, which significantly simplifies the analog design while maintaining professional-grade audio quality and reducing bill-of-materials cost to approximately sixty dollars.

## Fundamentals: Comparison Between Auditory Systems

To understand the project requirements and design decisions, we begin by analyzing the differences between human hearing, canine hearing, and the proposed system. This comparative analysis reveals both the biological limitations we seek to overcome and the engineering constraints that define practical boundaries.

### Human Auditory Characteristics

The human auditory system presents well-defined characteristics that have been extensively studied in psychoacoustic research. The audible frequency band extends from $20\text{Hz}$ to approximately $20\text{kHz}$ in young adults, though this upper limit degrades with age, typically reaching only $12$ to $15\text{kHz}$ by age sixty. Maximum sensitivity occurs in the range between $2\text{kHz}$ and $4\text{kHz}$, which corresponds to the fundamental frequencies and first formants of human speech, reflecting evolutionary optimization for verbal communication.

The dynamic range of human hearing, measured as the difference between the threshold of hearing and the threshold of pain, reaches approximately $120\text{dB}$. This remarkable range allows us to perceive sounds from the faint rustle of leaves at 10 dB **SPL** (**S**ound **P**ressure **L**evel) to the roar of jet engines at $130\text{dB}$ **SPL**. However, this sensitivity comes with vulnerability to acoustic trauma from prolonged exposure to high-intensity sounds above $85\text{dB}$ **SPL**.

Human spatial localization capability depends fundamentally on two binaural mechanisms that exploit the separation between our ears. The first mechanism is **I**nteraural **T**ime **D**ifference, **ITD**, which represents the difference in sound arrival time between the two ears. For a sound source positioned laterally, considering the typical interaural separation of approximately $17\text{cm}$ in adult humans, we can calculate the maximum **ITD**:

$$\text{**ITD**}_{\text{max}} = \frac{d}{v_{\text{sound}}} = \frac{0.17 \text{ m}}{343 \text{ m/s}} \approx 495 \text{ μs}$$

This half-millisecond time difference is remarkably small, yet the human auditory system can detect differences as small as 10 microseconds, corresponding to angular discrimination of approximately one to two degrees in the frontal hemisphere. The sensitivity to **ITD** is greatest for frequencies below $1.5\text{kHz}$, where phase information remains unambiguous across the entire audible range.

The second mechanism is **I**nteraural **L**evel **D**ifference, **ILD**, which represents the difference in sound intensity between the ears due to the acoustic shadow cast by the head. This effect becomes significant when the wavelength of sound is smaller than or comparable to head dimensions. For a spherical head of approximately $18\text{cm}$ diameter, this occurs at frequencies above approximately $2\text{kHz}$:

$$f_{\text{ILD}} = \frac{v_{\text{sound}}}{\pi d_{\text{head}}} = \frac{343 \text{ m/s}}{\pi \times 0.18 \text{ m}} \approx 607 \text{ Hz}$$

Above this frequency, the head increasingly attenuates sound reaching the far ear, creating level differences that can exceed 20 dB for pure lateral sources at high frequencies. The brain integrates both **ITD** and **ILD** cues to create a robust spatial representation, with **ITD** dominating at low frequencies and **ILD** at high frequencies.

Human angular resolution varies significantly with direction. In the frontal region, where both ears receive similar stimulation and head movements can disambiguate front-back confusions, we achieve resolution of approximately one degree. This exceptional acuity degrades to approximately $10$ to $15$ degrees in lateral and posterior regions, where cone of confusion effects and reduced binaural information limit performance. Figura @fig-human-auditory illustrates these concepts.

:::{#fig-human-auditory
[Illustration of the human auditory system showing the 17cm interaural separation, with visualization of sound waves arriving from different angles and visual indication of **ITD** and **ILD** mechanisms](../images/human_hearing.webp){#fig-human-auditory}
:::

### Canine Auditory Characteristics

Canine hearing presents characteristics that make it superior to human hearing in specific aspects, reflecting evolutionary adaptations to different ecological niches. The frequency band extends from approximately $40\text{Hz}$ to $60\text{kHz}$, with some breeds capable of detecting frequencies approaching $65\text{kHz}$. This extended high-frequency response enables dogs to perceive ultrasonic vocalizations from prey animals and potentially detect some electronic devices that emit ultrasonic noise.

Maximum sensitivity occurs between $8\text{kHz}$ and $16\text{kHz}$, approximately one octave above peak human sensitivity. This shift reflects optimization for detecting higher-pitched sounds, including rodent vocalizations and rustling movements in vegetation. The dynamic range is similar to humans at approximately $100\text{dB}$, though dogs show greater sensitivity to faint sounds, with hearing thresholds $4$ to $6\text{dB}$ lower than human thresholds across most of the audible spectrum.

The separation between canine ears varies considerably with breed, typically ranging from $5\text{cm}$ in small breeds to $10\text{cm}$ in large breeds. Although smaller than human interaural separation, dogs compensate through independent pinnae mobility. Each external ear can rotate through approximately $180$ degrees under precise muscular control, allowing dogs to actively scan the acoustic environment and effectively increase their spatial sampling aperture. This mobility provides dogs with dynamic beamforming capability that humans lack, partially offsetting the geometric disadvantage of smaller baseline.

Canine angular resolution in azimuth is estimated at approximately $5$ degrees under optimal conditions, somewhat inferior to frontal human resolution but superior to human performance in lateral directions. The ability to rapidly orient pinnae toward sound sources provides dogs with faster spatial acquisition than humans, who must rely primarily on head movements for spatial disambiguation.

### Proposed System Specifications

The system we are designing seeks to combine the best characteristics of biological auditory systems while adding capabilities enabled by digital signal processing that have no biological analogue. The design philosophy emphasizes practical utility over theoretical perfection, focusing on capabilities that provide clear value for the intended applications of sound source localization, event detection, and high-quality audio recording.

The useful frequency band will extend from $20\text{Hz}$ to $20\text{kHz}$, covering the complete audible range with flat response. While this lacks the ultrasonic extension of canine hearing, it provides complete coverage of acoustically relevant content for our target applications. The vast majority of informative content in explosions, aircraft noise, and speech occurs below $10\text{kHz}$, with harmonics extending to $20\text{kHz}$. Even for ultrasonic events such as bat echolocation or pneumatic leaks, the presence of harmonics within the audible band often provides sufficient detection capability.

The target dynamic range is $120\text{dB}$, matching human capability and allowing capture of sounds from very weak ($20\text{dB **SPL**}$, equivalent to a quiet room) to extremely intense ($140\text{dB **SPL**}$, near aircraft engines). The frequency response will be optimized to remain flat within ±$2\text{dB}$ from $100\text{Hz}$ to $16\text{kHz}$, degrading gradually to ±$3\text{dB}$ at $20\text{kHz}$, providing faithful representation of sound sources without coloration.

The target angular resolution is $3$ to $5$ degrees in azimuth and $5$ to $10$ degrees in elevation. Although the system lacks physical mobility like canine pinnae, we compensate through digital beamforming, which allows electronically steering array sensitivity across the entire spatial hemisphere with millisecond update rates. This provides spatial tracking capability that exceeds biological systems for rapidly moving sources.

The system latency target is less than $20\text{ms}$ from acoustic event to digital output, enabling real-time monitoring and triggering applications. Processing will occur entirely on the embedded microcontroller, eliminating dependence on external computing resources and enabling standalone operation.

## Microphone Array Geometry

The choice of microphone array geometry fundamentally determines the spatial localization capabilities and dictates many subsequent design decisions. The proposed configuration consists of two blocks of three microphones each, arranged vertically and separated horizontally, creating what we term a binaural array architecture.

### Topology and Dimensions **TODO: New Topology with canine inspiration**

Each vertical block contains three microphones uniformly spaced by 15 millimeters center-to-center, resulting in a total aperture of 30 millimeters per block. The two blocks are separated horizontally by a distance of 100 millimeters, measured between the center microphones of each block. This creates a configuration that replicates the essential geometry of binaural hearing while adding a vertical dimension absent in simple two-microphone systems.

This geometry was selected through iterative analysis of several competing requirements. First, the vertical arrangement of microphones in each block enables beamforming in the elevation plane through delay-and-sum or more sophisticated adaptive techniques. Second, the horizontal separation between blocks provides sufficient baseline for exploiting both **ITD** and ILD cues to determine azimuth, similar to biological binaural hearing. Third, the compact overall dimensions (100 mm × 30 mm) permit mounting on a single printed circuit board without mechanical complexity.

The choice of 15 millimeter vertical spacing represents a compromise between spatial resolution and aliasing. Tighter spacing would reduce aliasing frequency but decrease angular resolution. Wider spacing would improve resolution but introduce ambiguities at lower frequencies. The selected spacing provides acceptable performance across the frequency range of interest.

The 100 millimeter horizontal separation was chosen to approximate the effective acoustic baseline of small animals while remaining compatible with standard **PCB** manufacturing. This separation provides **ITD** values up to approximately 290 microseconds for pure lateral sources, sufficient for unambiguous localization using correlation-based methods across the full audible band.

**[SVG DIAGRAM 3: Front and top view of the array showing the positions of the 6 microphones. Front view shows the two vertical blocks separated by 10cm. Top view shows the 10cm baseline. Include coordinate system with x, y, z axes clearly marked and microphone numbering]**

### Spacing Calculation and Spatial Aliasing 

A critical concept in microphone array design is spatial aliasing, directly analogous to temporal aliasing in discrete-time signal processing. When the spacing between array elements exceeds half the wavelength of the incoming sound, ambiguities arise in determining the direction of arrival. Multiple spatial angles can produce identical phase relationships across the array, creating what are termed grating lobes in the array response pattern.

To avoid spatial aliasing, the spacing between adjacent elements must satisfy:

$$d < \frac{\lambda_{\text{min}}}{2} = \frac{v_{\text{sound}}}{2 f_{\text{max}}}$$

For our maximum frequency of interest of 20 kHz, this yields:

$$\lambda_{\text{min}} = \frac{343 \text{ m/s}}{20000 \text{Hz}} = 17.15 \text{mm}$$

Therefore, the maximum spacing to avoid aliasing is:

$$d_{\text{max}} = \frac{17.15 \text{mm}}{2} = 8.58 \text{mm}$$

Our chosen spacing of 15 millimeters exceeds this limit by a factor of 1.75, which means we will experience spatial aliasing above approximately:

$$f_{\text{aliasing}} = \frac{v_{\text{sound}}}{2 \times d} = \frac{343 \text{ m/s}}{2 \times 0.015 \text{ m}} \approx 11.4 \text{kHz}$$

This represents a conscious design trade-off. Spatial aliasing above 11.4 kHz creates ambiguities in direction of arrival estimation, but several factors mitigate this limitation. First, most acoustic events of interest (speech, explosions, vehicle noise) contain their principal informative content below 10 kHz. Second, even when aliasing occurs, the energy detection and beamforming gain remain valid, only the precise angle-of-arrival becomes ambiguous. Third, the horizontal baseline of 100 millimeters provides an independent measurement at a different spatial frequency that helps resolve some ambiguities.

For applications requiring unambiguous localization across the full 20 kHz band, the vertical spacing would need to be reduced to approximately 8 millimeters. However, this would increase manufacturing difficulty and reduce the total vertical aperture, degrading elevation resolution. The chosen design prioritizes practical manufacturability and optimizes performance in the most critical frequency range.

### Local Beamforming Capabilities

Each vertical block of three microphones forms a uniform linear array (ULA) capable of performing beamforming in the elevation plane. For a ULA with $N$ elements spaced by distance $d$, the far-field array pattern when steering to angle $\theta_0$ from the array normal is given by:

$$
H(\theta, \omega) = \sum_{n=0}^{N-1} w_n e^{j \omega \tau_n}
$$

where $w_n$ are complex weights and $\tau_n = (n d \sin(\theta))/v_{\text{sound}}$ is the acoustic delay to element $n$. For a simple delay-and-sum beamformer with uniform weighting ($w_n = 1/N$), this simplifies to:

$$
H(\theta, \omega) = \frac{1}{N} \frac{\sin(N \omega d \sin(\theta) / (2 v_{\text{sound}}))}{\sin(\omega d \sin(\theta) / (2 v_{\text{sound}}))}
$$

The directivity pattern exhibits a main lobe centered at the steering angle $\theta_0$ and side lobes at angles where the path length differences equal integer multiples of the wavelength. The main lobe width at the -3 dB points is approximated by:

$$
\Delta\theta_{3\text{dB}} \approx \frac{0.886 \lambda}{(N-1)d}
$$

For our vertical array with $N=3$, $d=15$ millimeters, at a frequency of 1 kHz where $\lambda = 343$ millimeters:

$$
\Delta\theta_{3\text{dB}} \approx \frac{0.886 \times 343 \text{ mm}}{2 \times 15 \text{ mm}} \approx 10.1°
$$

This represents the angular resolution in elevation at 1 kHz. The beamwidth narrows at higher frequencies, reaching approximately 2.5 degrees at 4 kHz, our frequency of peak array gain. The directivity gain, which represents the improvement in signal-to-noise ratio achieved by coherent summation, is approximately:

$$
G_{\text{array}} = 10 \log_{10}(N) = 10 \log_{10}(3) \approx 4.8 \text{ dB}
$$

This 4.8 dB gain applies when the desired signal arrives from the steering direction and noise is spatially uncorrelated. In practice, gains of 3 to 4 dB are typical accounting for microphone mismatch and finite coherence of the desired signal.

**[SVG DIAGRAM 4: Polar pattern of vertical beamforming showing the main lobe and side lobes at three frequencies: 500 Hz (wide beam), 2 kHz (medium), and 8 kHz (narrow). Include angular scale in degrees and magnitude scale in dB]**

### Binaural Analysis for Azimuth Determination

The 100 millimeter horizontal separation between left and right blocks enables binaural analysis for azimuth determination. For a sound source at azimuth angle $\phi$ measured from the frontal direction, the maximum interaural time difference for pure lateral incidence ($\phi = 90°$) is:

$$
\text{**ITD**}_{\text{max}} = \frac{d_{\text{separation}}}{v_{\text{sound}}} = \frac{0.10 \text{ m}}{343 \text{ m/s}} \approx 291 \text{ μs}
$$

For arbitrary azimuth angles, the **ITD** can be approximated under far-field conditions by:

$$
\text{**ITD**}(\phi) = \frac{d_{\text{separation}}}{v_{\text{sound}}} \sin(\phi)
$$

This relationship is valid when the source distance greatly exceeds the array dimensions, allowing treatment of the wavefront as planar. For sources within approximately two meters, near-field corrections become necessary to account for wavefront curvature.

The angular resolution achievable through **ITD** analysis depends on the minimum detectable time difference, which is limited by sampling rate, signal-to-noise ratio, and correlation processing gain. With a sampling rate of 48 kHz providing temporal resolution of approximately 21 microseconds, and assuming signal-to-noise ratio of 20 dB enabling detection of time delays to approximately 10 microseconds through correlation processing, the theoretical angular resolution is:

$$
\Delta\phi_{\text{min}} = \arcsin\left(\frac{v_{\text{sound}} \times \Delta t_{\text{min}}}{d_{\text{separation}}}\right) = \arcsin\left(\frac{343 \text{ m/s} \times 10 \times 10^{-6} \text{ s}}{0.10 \text{ m}}\right) \approx 2.0°
$$

This calculation indicates that our geometry is capable of achieving the target angular resolution of 3 to 5 degrees with margin. In practice, reverberation, competing sources, and finite signal bandwidth will degrade performance from this theoretical limit, but the fundamental geometry provides adequate capability.

The interaural level difference also contributes to azimuth determination, particularly at high frequencies where the wavelength becomes comparable to the array dimensions. For our 100 millimeter separation, ILD becomes significant above approximately:

$$
f_{\text{ILD}} = \frac{v_{\text{sound}}}{\pi d_{\text{separation}}} = \frac{343 \text{ m/s}}{\pi \times 0.10 \text{ m}} \approx 1.1 \text{ kHz}
$$

Above this frequency, each vertical block begins to shadow the other for lateral sources, creating level differences that complement **ITD** cues. The magnitude of ILD increases with frequency and angle, potentially reaching 10 to 15 dB for high-frequency pure lateral sources.

**[SVG DIAGRAM 5: Top view showing plane waves arriving at different azimuth angles (0°, 30°, 60°, 90°) with visualization of path length differences between left and right blocks. Include time delay annotations and graphical wavefront representations]**

## Digital Microphone Architecture

The decision to adopt an all-digital microphone architecture using **I²S**/**PDM** **MEMS** devices represents a fundamental design choice with far-reaching implications for the entire system. This section explains the rationale for this decision and details the specific implementation.

### Rationale for Digital Microphones

Traditional analog microphone systems require careful attention to numerous potential noise sources in the signal chain. The microphone output, typically in the millivolt range, must be amplified, filtered, and transmitted to an analog-to-digital converter while maintaining signal integrity. Each stage introduces noise, and the analog routing on the **PCB** acts as an antenna for electromagnetic interference.

Digital **MEMS** microphones integrate the acoustic transducer, preamplifier, and ADC within a single miniature package, typically $3.5 \times 2.6 \times 1.0 \text{mm}$. The output is a digital bitstream in either **PDM** (Pulse Density Modulation) or I²S (Inter-IC Sound) format. This digital output provides several critical advantages for our application.

First, digital signals are immune to noise pickup during **PCB** routing. The same trace that would capture millivolts of noise when carrying analog signals can route digital signals with complete immunity to interference, as long as noise remains below the switching threshold. This dramatically simplifies **PCB** layout and eliminates entire categories of design challenges related to ground loops, crosstalk, and EMI.

Second, digital microphones eliminate the need for an external codec with its associated power supply noise requirements, reference voltage generation, and clock distribution challenges. The STM32H753 microcontroller can directly interface with the digital outputs, reducing component count and cost.

Third, synchronization between channels is guaranteed by the shared clock and data lines. All microphones sample simultaneously when driven by the same clock, providing the phase coherence essential for beamforming without the clock skew issues that plague multiple independent ADCs.

Fourth, digital microphones provide excellent performance at moderate cost. Modern **MEMS** microphones achieve **S**ignal-to-**N**oise **R**atio, **SNR** of $64\text{dB}$ to $69\text{dB}$, dynamic range of $100\text{dB}$ to $120\text{dB}$, and flat frequency response from $20\text{Hz}$ to $20\text{kHz}$, performance that would require careful design and expensive components in an analog implementation.

### Selected Microphone: Infineon IM69D130

After surveying the market for digital **MEMS** microphones suitable for professional audio applications, we selected the Infineon [IM69D130](https://www.infineon.com/part/IM69D130) as the primary candidate. This microphone was chosen based on several key specifications that align well with our requirements.

The [IM69D130](https://www.infineon.com/part/IM69D130) provides **SNR** of $69\text{dB}$ A-weighted, which is at the top of the current **MEMS** microphone market. This high **SNR** is critical for maintaining our target dynamic range and enabling detection of weak acoustic signals in the presence of self-noise. The acoustic overload point is 130 dB **SPL**, providing 10 dB of headroom above our target maximum level and ensuring freedom from clipping even during transient peaks.

:::{.callout-note}
**What is "A-weighted" (dBA)?**

"A-weighted" (dBA) is a standard filter used in sound level measurement that **mimics the perception of the human ear.** Our ears are most sensitive to mid-range frequencies (like speech) and less sensitive to very low bass and very high treble.

The **dBA** measurement reflects this curve, providing a more accurate reading of *perceived* loudness rather than just raw physical sound pressure. It is the global standard for environmental noise and hearing safety assessments.
:::

The frequency response is specified as $±1\text{dB}$ from $20\text{Hz}$ to $10\text{kHz}$, degrading to $±3\text{dB}$ at $20\text{kHz}$. This flat response across the audible band is essential for faithful audio capture and enables accurate spectrum analysis for event detection. The low-frequency response extending to $20\text{Hz}$ is particularly noteworthy, as many competing microphones exhibit high-pass characteristics that attenuate sub-$100\text{Hz}$ content where explosion and aircraft signatures concentrate significant energy.

The microphone outputs **PDM** (**P**ulse **D**ensity **M**odulation) data, a $1-bit$ high-speed digital representation of the audio signal. The **PDM** clock can range from $1.0 \text{MHz}$ to $3.25 \text{MHz}$, with typical operation at $2.4\text{MHz}$ providing $48\text{kHz}$ audio bandwidth. The [STM32H753](https://www.st.com/en/microcontrollers-microprocessors/stm32h743-753/documentation.html) includes hardware **DFSDM** (**D**igital Filter for Sigma-Delta Modulators) peripherals specifically designed to convert **PDM** bitstreams to **P**ulse-**C**ode **M**odulation, **PCM** samples, eliminating software processing overhead.

Power consumption is $650\mu\text{A}$ at $1.8\text{V}$ supply, totalling approximately $1.2 \text{mW}$ per microphone. For six microphones, total power is only $7.2\text{mW}$, negligible compared to the microcontroller and other system components.

The small package size $3.76 \times 2.95 \times 1.00 \text{mm}$ permits dense placement on the **PCB** while maintaining our $15\text{mm}$ element spacing. The top-port acoustic configuration allows bottom mounting on the **PCB** with an acoustic port through the board, simplifying mechanical design.

**[SVG DIAGRAM 6: Cross-sectional diagram of the IM69D130 showing the **MEMS** membrane, integrated ADC, and digital output. Include signal path from acoustic pressure through membrane deflection, capacitive sensing, sigma-delta modulation, to **PDM** output]**

### Alternative Considerations

We evaluated several alternative microphones during the selection process. The TDK InvenSense ICS-43434 offers similar specifications (65 dB SNR, 116 dB AOP) at lower cost but with I²S output rather than PDM. While I²S simplifies interface in some contexts, it consumes more GPIO pins on the microcontroller (requiring clock, word select, and data lines per stereo pair) compared to **PDM** which uses only clock and data with time-division multiplexing. For a six-channel system, this difference is significant.

The Knowles SPH0645LM4H-B provides 65 dB **SNR** with I²S output and has been widely used in hobbyist audio projects, establishing a track record of reliability. However, the frequency response shows earlier roll-off than the IM69D130, with -3 dB point around 15 kHz rather than 20 kHz. For applications requiring full audible bandwidth, the Infineon part provides better performance.

The STMicroelectronics MP34DT06J offers 64 dB **SNR** with **PDM** output at attractive cost but in a less convenient package size and with less extensive documentation. For a professional development, the additional cost of the Infineon part is justified by better support resources and clearer specifications.

I feel this pain in my cheapskate bones.

## Engineering Rationale: ESP32-P4 vs. STM32H7

My first impulse was to use the ESP32-P4, however, a deeper architectural analysis revealed that the initial cost advantages of the Espressif SoC would be negated by increased hardware complexity and potential risks to signal integrity. While the dual-core RISC-V architecture running at 400 MHz is impressive on paper, the specific requirements of a six-channel PDM microphone array for precise sound source localization align far better with the specialized peripherals found in the STM32H7 series. I feel this pain in my bones.

### The PDM Interface and Hardware Filtering
The most significant technical differentiator is the method of handling Pulse Density Modulation (PDM) data. The project relies on PDM microphones for their noise immunity and simplicity. The STM32H753 features the DFSDM (Digital Filter for Sigma-Delta Modulators) peripheral, a dedicated hardware block designed specifically to generate clock signals, receive 1-bit bitstreams, and perform decimation and filtering in hardware. This allows the capture of six synchronous audio channels directly into memory via DMA with virtually no CPU overhead.

In contrast, utilizing the ESP32-P4 for the same task introduces unnecessary friction. While the P4 supports PDM, efficiently managing six synchronized channels typically requires significant software intervention for decimation or restricts the clock topology. To achieve the same "clean" integration as the STM32, the ESP32 implementation would likely require external codecs or complex software decimation loops that consume valuable processing cycles needed for the beamforming algorithms.

### Clock Precision and Spatial Localization
For a system designed to detect the angle of arrival of sound waves, the stability of the audio clock is not merely a matter of fidelity but of geometric accuracy. Sound source localization algorithms, such as GCC-PHAT, rely on minute phase differences between microphones. The STM32H7 series includes dedicated audio PLLs known for low jitter, allowing the microcontroller to act as a reliable clock master for the array.

The ESP32 architecture historically exhibits higher internal PLL jitter, which introduces phase noise. While tolerable for standard voice recording, this phase noise translates directly into angular error in a compact beamforming array. Mitigating this on the ESP32-P4 would require inverting the clock topology to use an external high-precision crystal oscillator as the master, forcing the MCU into slave mode. This adds components to the Bill of Materials and complicates the PCB routing, whereas the STM32 handles this natively and robustly.

### Development Ecosystem and PCB Simplicity
The choice of the STM32H7 enables a direct "star" topology where the microcontroller connects directly to the microphones without intermediate silicon. This results in the cleanest possible HAT layout, reducing the risk of electromagnetic interference and minimizing points of failure. Furthermore, the firmware development benefits from the maturity of the ARM Cortex-M7 ecosystem, specifically the CMSIS-DSP libraries which offer highly optimized, battle-tested floating-point math functions essential for the real-time FFTs and correlation matrices used in the project. Porting these complex DSP chains to the newer RISC-V vector extensions represents a schedule risk that outweighs the unit cost savings of the processor.



## Acquisition System Architecture

The signal acquisition chain transforms acoustic pressure variations into digital representations suitable for processing and analysis. In our all-digital architecture, this transformation occurs primarily within the microphone packages themselves, with the microcontroller serving to collect, filter, and format the digital bitstreams.

### Block 1: Six IM69D130 **MEMS** Microphones

The six IM69D130 microphones are arranged on the **PCB** according to the geometry discussed previously, with three forming the left block and three forming the right block. Each microphone operates independently but shares common clock and power rails to ensure synchronization.

The **PDM** clock for all six microphones is generated by the STM32H753 and distributed via a tree topology with controlled impedance traces. Operating at 2.4 MHz, this clock defines the sampling instant for all microphones simultaneously. The one-bit **PDM** data from each microphone flows on a dedicated data line back to the microcontroller.

The **PDM** encoding represents the audio signal through density modulation of the bitstream. High audio levels produce bit patterns with more ones than zeros, while low levels produce more zeros than ones. A 1 kHz sine wave at moderate level might produce a pattern like 111110111101111011110..., while a DC offset would produce either all ones or all zeros depending on sign. This representation trades temporal resolution for amplitude resolution, requiring digital filtering to recover conventional PCM samples.

The microphones require 1.8 volt supply, provided by a low-dropout regulator dedicated to this function. Current consumption of 650 microamperes per microphone yields total current of 3.9 milliamperes, well within the capability of small LDO regulators. Decoupling capacitors of 100 nanofarads are placed within 2 millimeters of each microphone power pin to suppress high-frequency supply noise that could couple into the **MEMS** transducer through substrate conduction.

**[SVG DIAGRAM 7: Block diagram showing the six IM69D130 microphones connected to the STM32H753. Show the shared **PDM** clock distribution tree and individual data return paths. Include power supply connections and decoupling capacitors]**

### Block 2: STM32H753 Microcontroller with DFSDM

The STM32H753ZIT6 microcontroller serves as the central processor for the system, handling acquisition, processing, and communication functions. This device integrates several peripherals specifically designed for multi-channel audio applications that make it particularly suitable for our requirements.

The DFSDM (Digital Filter for Sigma-Delta Modulators) peripheral is the critical feature for **PDM** microphone interfacing. The STM32H753 includes four DFSDM channels, each capable of processing two **PDM** bitstreams through time-division multiplexing, providing capacity for eight microphones total. Each DFSDM channel includes:

A configurable sinc filter that converts the **PDM** bitstream to PCM samples. The sinc filter implements a moving average over a window of **PDM** samples, performing the mathematical equivalent of decimation and anti-aliasing filtering. The decimation factor determines the relationship between **PDM** clock frequency and output sample rate. For our configuration with 2.4 MHz **PDM** clock and desired 48 kHz output:

$$
\text{Decimation factor} = \frac{f_{\text{PDM}}}{f_{\text{sample}}} = \frac{2400000 \text{ Hz}}{48000 \text{ Hz}} = 50
$$

An integrator stage that can implement additional filtering beyond the basic sinc response. The integrator order can be configured from 1 to 5, with higher orders providing steeper roll-off in the anti-aliasing filter at the cost of increased group delay. We typically use third-order integration as a compromise between filter performance and latency.

An offset correction circuit that removes DC bias from the signal path. **MEMS** microphones exhibit small DC offsets in their **PDM** output due to mechanical asymmetries and circuit mismatches. The DFSDM can measure and subtract these offsets automatically.

A direct memory access (DMA) interface that transfers converted samples to RAM without CPU intervention. Each DFSDM channel can trigger DMA transfers at the output sample rate, filling circular buffers that the CPU processes when complete blocks are available. This architecture allows the CPU to remain in low-power sleep states during quiescent periods, waking only when processing is required.

The STM32H753 operates at 480 MHz with a Cortex-M7 core featuring superscalar dual-issue pipeline and single-precision floating-point unit. The manufacturer specifies performance of approximately 1.025 DMIPS per MHz, yielding total performance of approximately 492 DMIPS. However, this figure represents best-case performance on carefully optimized benchmarks. For mixed workloads including audio processing, realistic sustained performance is approximately 300 to 400 MIPS, which we use for capacity planning.

The device includes 1024 kilobytes of SRAM distributed across several domains with different access speeds. For audio processing, we primarily use the tightly-coupled data memory (DTCM) which provides single-cycle access for optimal loop performance. The large memory capacity allows implementing substantial buffering to absorb timing variations and enable batch processing for efficiency.

### Data Flow and Buffer Management

The data flow from microphones to processed output follows a carefully orchestrated pipeline designed to minimize latency while maintaining processing efficiency. Understanding this pipeline is critical to appreciating the real-time performance capabilities of the system.

Each DFSDM channel outputs 48 kHz samples in 24-bit format, though the effective resolution is lower due to the inherent noise of the **MEMS** transducer and quantization in the **PDM** modulator. The six channels (three pairs through three DFSDM modules) produce:

$$
\text{Data rate} = 6 \text{ channels} \times 3 \text{ bytes/sample} \times 48000 \text{ samples/s} = 864000 \text{ bytes/s} \approx 844 \text{ kB/s}
$$

This data rate is modest by modern standards but requires continuous attention to avoid buffer overruns. The DFSDM peripherals transfer samples via DMA to circular buffers in SRAM. We configure buffers of 512 samples per channel (10.67 milliseconds at 48 kHz), providing adequate time for processing while limiting latency.

The DMA controller is configured to generate an interrupt when each half-buffer completes. This half-buffer scheme enables the CPU to process one half while the DMA fills the other, implementing a classic double-buffering pattern. The interrupt service routine triggers processing tasks that consume the completed half-buffer.

Processing occurs in 256-sample blocks (5.33 milliseconds). This block size represents a compromise between processing efficiency (larger blocks reduce loop overhead and enable better use of instruction cache) and latency (smaller blocks reduce delay). At 48 kHz, 256 samples provides adequate frequency resolution for spectrum analysis while maintaining sub-10 millisecond algorithmic latency.

After processing, the data is formatted for transmission via USB according to the selected operating mode. The USB Audio Class 2.0 interface consumes processed audio from circular buffers using isochronous transfers that guarantee bandwidth but do not retransmit lost packets. Buffer management must ensure that data is always available when the USB host requests it, requiring careful tuning of buffer depths to absorb timing jitter in the USB polling.

**[SVG DIAGRAM 8: Data flow diagram showing the complete path from **PDM** bitstream through DFSDM filtering, DMA transfer to SRAM buffers, CPU processing blocks (beamforming, DOA, event detection), and final USB output. Include timing annotations showing latency at each stage]**

### Clock Architecture

All timing in the system derives from a single master crystal oscillator, ensuring frequency coherence across all audio channels and preventing beat frequencies that would occur with independent clocks. The clock architecture trades simplicity for flexibility, using integer division from the master frequency to derive all other timing signals.

The master oscillator operates at 25 MHz, a standard frequency for microcontrollers that provides convenient integer relationships to common audio sample rates. The STM32H753 internal phase-locked loops multiply this frequency to generate the 480 MHz core clock and other peripheral clocks. For **PDM** clock generation:

$$
f_{\text{PDM}} = \frac{f_{\text{master}} \times \text{PLL multiplier}}{\text{Divider}} = \frac{25 \text{ MHz} \times 96}{1} \div 1000 = 2.4 \text{ MHz}
$$

The relationship between **PDM** clock and sample rate is fixed by the DFSDM decimation factor, ensuring that sample rate tracks any variations in the master clock. This means temperature drift or aging of the crystal affects all channels identically, maintaining phase coherence.

The crystal oscillator selected is a standard AT-cut crystal with ±20 ppm initial tolerance and ±5 ppm temperature stability over the operating range. This translates to frequency variation of:

$$
\Delta f = 48000 \text{ Hz} \times 25 \times 10^{-6} = 1.2 \text{ Hz}
$$

This frequency error is negligible for audio applications, affecting pitch by an imperceptible amount. For spatial localization, the absolute frequency is irrelevant as only phase relationships between channels matter, and these are preserved by the common clock.

Phase jitter represents rapid short-term variations in clock period that can degrade signal-to-noise ratio by introducing timing uncertainty in the sampling instant. High-quality crystal oscillators exhibit phase jitter in the range of 5 to 20 picoseconds RMS. For our application, 10 picoseconds RMS is typical. The effect on effective number of bits can be estimated by:

$$
\text{ENOB loss} = -20 \log_{10}(2\pi f_{\text{max}} t_{\text{jitter}}) / 6.02
$$

For maximum audio frequency of 20 kHz and jitter of 10 picoseconds:

$$
\text{ENOB loss} = -20 \log_{10}(2\pi \times 20000 \times 10 \times 10^{-12}) / 6.02 = -20 \log_{10}(1.26 \times 10^{-6}) / 6.02 \approx 0.16 \text{ bits}
$$

This negligible loss confirms that clock jitter does not limit system performance for audio frequencies. Only at ultrasonic frequencies above 100 kHz would jitter become significant.

### Block 3: USB Audio Class 2.0 Interface

The interface to the host computer (Raspberry Pi or general-purpose PC) employs USB 2.0 High-Speed operating at 480 Mbps. The USB Audio Class 2.0 (UAC 2.0) protocol eliminates proprietary drivers, enabling plug-and-play operation on Linux, macOS, and Windows 10/11 with built-in USB audio drivers.

UAC 2.0 defines a standard interface for multi-channel audio devices, specifying formats, sample rates, and control commands. The host enumerates the device during connection, discovering available audio formats by reading descriptors. Our device advertises support for:

6 channels of input (capture) at 48 kHz, 24-bit resolution for raw streaming mode
2 channels of input at 48 kHz, 24-bit resolution for processed binaural mode
Both modes with optional metadata sideband channel for direction-of-arrival and level information

The USB High-Speed isochronous transport provides guaranteed bandwidth allocation with fixed timing but no retransmission of lost packets. This matches the requirements of real-time audio where timeliness is more important than perfect reliability. Buffer underruns appear as clicks or gaps rather than indefinite delays.

For six channels at 48 kHz and 24 bits:

$$
\text{Required bandwidth} = 6 \times 3 \text{ bytes} \times 48000 \text{ Hz} = 864 \text{ kB/s} = 6.9 \text{ Mbps}
$$

This represents only 1.4% utilization of the 480 Mbps USB link, providing enormous margin for timing jitter and overhead. The actual USB transfer is organized into 1 millisecond frames, each carrying 864 bytes of audio data plus protocol overhead. The isochronous endpoint is configured with double-buffering to tolerate host latency.

Latency in the USB transport comes from buffering required to match the asynchronous clocks of the audio system and USB bus. Although both nominally operate at defined rates (48 kHz audio, 1000 frames/sec USB), the actual frequencies differ slightly due to crystal tolerances. The audio clock is master, and the USB side must buffer to adapt. Typical implementations buffer 2 to 5 milliseconds, which we account for in overall latency budget.

The UAC 2.0 implementation uses the TinyUSB stack, an open-source USB device stack specifically designed for embedded systems. TinyUSB provides well-tested implementations of UAC 2.0 descriptors and transfer management, significantly reducing development effort compared to writing USB code from scratch. The stack has been validated with thousands of different USB host implementations, providing confidence in compatibility.

**[SVG DIAGRAM 9: Block diagram of the complete architecture showing the 6 microphones, STM32H753 with internal DFSDM blocks, USB PHY, and connection to Raspberry Pi. Include data rate annotations at each interface and highlight the power supply distribution]**

## Rationale for Choosing the Three TI Voltage Regulators

The selection of three Texas Instruments (TI) low-dropout (LDO) voltage regulators—one TPS7A4700 for generating 3.3V from the Raspberry Pi's 5V rail and two TPS7A33 for separate 1.8V supplies (one for the microphones and one for the STM32H753's analog sections)—is driven by the project's stringent requirements for ultra-low-noise power delivery, which is essential for achieving high-fidelity audio capture, precise spatial localization, and low-latency processing. 

Although the Raspberry Pi provides 5V and 3.3V rails directly through the GPIO header, these are prone to ripple and high-frequency noise (typically 50-100 mV peak-to-peak up to 1 MHz) from its switching regulators, CPU activity, USB peripherals, and other components, which can easily couple into sensitive elements like the IM69D130 MEMS microphones and the STM32H753's DFSDM peripheral. This noise would degrade the system's targeted 105-120 dB dynamic range, 69 dB SNR, and flat 20 Hz-20 kHz frequency response, potentially introducing artifacts that compromise the 3-5° angular resolution in azimuth or the sub-20 ms latency for detecting events like explosions or aircraft. 

The TI regulators address this with exceptional performance, offering ultra-low output noise (<4.17 µV RMS from 10 Hz to 100 kHz) and high power supply rejection ratio (PSRR >70 dB up to 1 MHz), effectively isolating the project's components from the Pi's noisy rails while ensuring stable voltages for faithful signal processing. By providing separate clean rails, they maintain phase coherence across the microphones for accurate interaural time and level differences (ITD/ILD), align with the "all-digital low-noise architecture" philosophy, and support the overall bill-of-materials cost target of approximately $60, making them a deliberate choice to prioritize audio fidelity and robustness over simplicity in this HAT design.

## Printed Circuit Board Design

The **PCB** design translates the block diagram architecture into physical reality, and the quality of this translation directly determines the performance of the manufactured system. While our all-digital architecture dramatically simplifies many aspects compared to analog designs, careful attention to power distribution, signal integrity, and electromagnetic compatibility remains essential.

### Four-Layer Stack-up

We employ a four-layer **PCB** stack-up that balances cost against performance. The layer arrangement from top to bottom is:

Layer 1 (Top): Components and signal routing
Layer 2 (Inner): Continuous ground plane
Layer 3 (Inner): Power distribution for 3.3V and 1.8V rails
Layer 4 (Bottom): Additional ground plane and secondary signals

This stack-up provides several key benefits. The ground plane on Layer 2 sits immediately beneath the components and signal traces on Layer 1, creating a controlled impedance environment and providing a low-impedance return path for all signals. The capacitance between Layer 1 traces and Layer 2 ground is:

$$
C_{\text{trace}} = \frac{\epsilon_0 \epsilon_r W L}{h}
$$

where $W$ is trace width, $L$ is length, $h$ is dielectric height (typically 0.2 millimeters), and $\epsilon_r$ is the dielectric constant of FR-4 (approximately 4.5). For a typical signal trace (0.15 millimeters wide, 20 millimeters long), this yields:

$$
C_{\text{trace}} = \frac{8.85 \times 10^{-12} \times 4.5 \times 0.15 \times 10^{-3} \times 20 \times 10^{-3}}{0.2 \times 10^{-3}} \approx 0.6 \text{ pF}
$$

This capacitance is small enough not to load signals significantly but large enough to provide some filtering of high-frequency noise.

The power plane on Layer 3 distributes supply voltages with low impedance, acting as a reservoir of charge to support transient current demands. Multiple capacitors scattered across the board connect between the power planes and ground, creating a distributed LC filter network that suppresses power supply noise across a wide frequency range.

The bottom ground plane on Layer 4 provides additional shielding and can be used for return paths of bottom-side signals if needed. In our design, Layer 4 is primarily ground plane with small areas devoted to signals that cannot be routed on the top layer due to density constraints.

Standard **PCB** thickness is 1.6 millimeters with 35 micrometer (1 ounce) copper on each layer. Controlled impedance traces are calculated for 50 ohms single-ended or 100 ohms differential, though few traces in our digital design require strict impedance control due to the moderate speeds involved (2.4 MHz **PDM** clock has rise times in the nanosecond range, far slower than the transit time across the board).

**[SVG DIAGRAM 10: Cross-section of the **PCB** showing the 4-layer stack-up with thickness dimensions. Show representative traces on Layer 1, the solid ground plane on Layer 2, power distribution on Layer 3, and bottom ground on Layer 4. Include dielectric thickness annotations]**

### Microphone Placement and Acoustic Considerations

The six microphones are placed according to the geometry specified earlier, with 15 millimeters vertical spacing and 100 millimeters horizontal separation. Each microphone footprint includes acoustic porting considerations since the IM69D130 uses a top-port configuration where sound enters through the top of the package but must reach the membrane inside.

We employ bottom-mounting of the microphones with acoustic holes drilled through the PCB. This approach places the microphones on the bottom side of the board with their top ports aligned to 1 millimeter diameter holes through all four layers. Sound travels through the hole to reach the microphone, and this acoustic path must be kept clear of solder mask and silkscreen that could partially obstruct the opening.

The hole diameter of 1 millimeter is chosen to match the microphone port while being large enough for reliable **PCB** fabrication. The acoustic impedance of the hole affects the low-frequency response. The hole acts as a mass-loaded compliance, forming a high-pass filter with corner frequency:

$$
f_c = \frac{c}{2\pi} \sqrt{\frac{S}{V L}}
$$

where $S$ is hole area, $V$ is back volume behind the microphone membrane (determined by package geometry, approximately 0.5 cubic millimeters), $L$ is effective hole length (PCB thickness plus end corrections, approximately 2 millimeters), and $c$ is the speed of sound. Evaluating:

$$
f_c = \frac{343}{2\pi} \sqrt{\frac{\pi (0.5 \times 10^{-3})^2}{0.5 \times 10^{-9} \times 2 \times 10^{-3}}} \approx 17 \text{ Hz}
$$

This corner frequency lies below the microphone's specified response, confirming that the acoustic porting does not limit low-frequency performance.

Each microphone is placed with a keep-out zone extending 5 millimeters in all directions to prevent acoustic reflections from nearby components. Ground plane is maintained beneath each microphone without interruption to provide shielding from substrate-conducted noise.

### Digital Signal Routing

Digital signals in our design include the **PDM** clock distribution to six microphones, six **PDM** data return lines, USB data differential pair, and various control signals to the microcontroller. While these signals operate at moderate frequencies where transmission line effects are typically negligible, we follow good practices to ensure signal integrity and minimize radiated emissions.

The **PDM** clock is distributed in a tree topology from the microcontroller to the six microphones. The microcontroller output drives a short trace to a branch point, from which three traces extend to the left block microphones and three to the right block microphones. Each branch is approximately 15 millimeters long, creating time-of-flight delay of:

$$
\tau = \frac{L}{v_{\text{prop}}} = \frac{15 \text{ mm}}{170 \text{ mm/ns}} \approx 88 \text{ ps}
$$

where $v_{\text{prop}} = c/\sqrt{\epsilon_{r,\text{eff}}}$ is the propagation velocity in microstrip with effective dielectric constant of approximately 3.5. This 88 picosecond skew between the nearest and farthest microphones is negligible compared to the 417 nanosecond period of the 2.4 MHz clock (representing 0.02% of a clock cycle), confirming that all microphones sample synchronously despite the tree distribution.

The **PDM** data lines return from each microphone to dedicated GPIO pins on the microcontroller. These traces are routed with moderate care to avoid crosstalk but do not require differential pairing or impedance control. The data changes on the falling edge of the **PDM** clock and is sampled by the microcontroller on the rising edge, providing half a clock period (208 nanoseconds) of setup time, far exceeding the few nanoseconds typically required.

The USB data signals (D+ and D-) form a differential pair that must be routed as matched-length, controlled-impedance transmission lines. USB 2.0 High-Speed specifies 90 ohms differential impedance. For our **PCB** stack-up, this is achieved with trace widths of approximately 0.15 millimeters and spacing of 0.12 millimeters. The traces are routed as short and direct as possible, with length matching within 0.5 millimeters to maintain phase alignment.

**[SVG DIAGRAM 11: **PCB** layout top view showing microphone positions (6 devices), STM32H753 placement, **PDM** clock distribution tree, **PDM** data return traces, and USB differential pair routing. Use color coding: microphones in green, digital signals in blue, power in red, ground in black]**

### Power Supply System

Power supply quality directly impacts microphone performance despite their digital outputs. The **MEMS** transducer remains an analog device, and supply noise can modulate the membrane bias voltage or couple into the internal circuitry, degrading signal-to-noise ratio. We therefore implement a carefully designed power distribution network despite the modest current requirements.

The system is powered from the USB 5 volt supply (VBUS), which typically exhibits moderate noise from the host computer's switching regulators. We condition this supply through three linear regulators to create clean rails for different subsystems:

**3.3V main rail**: Powers the STM32H753 digital core and I/O. Generated by a TPS7A4700 ultra-low-noise LDO rated for 1 amp output. The microcontroller draws approximately 200 milliamps at full processing load, well within the regulator capability. The TPS7A4700 provides PSRR (Power Supply Rejection Ratio) of greater than 70 dB at frequencies up to 1 MHz, effectively isolating the 3.3 volt rail from noise on the 5 volt input.

**1.8V microphone rail**: Powers the six IM69D130 microphones and is generated by a TPS7A33 ultra-low-noise LDO rated for 300 milliamps. Total microphone current is only 3.9 milliamps, so the regulator operates far below its maximum, improving PSRR and thermal stability. The TPS7A33 achieves output noise of less than 10 microvolts RMS in the 10 Hz to 100 kHz band, which is negligible compared to the microphone self-noise.

**1.8V microcontroller analog rail**: The STM32H753 includes analog peripherals (ADCs used for monitoring functions) that require low-noise supply separate from the digital rails. This is provided by a separate TPS7A33 regulator dedicated to the microcontroller analog supply pins.

Each regulator includes input and output capacitors specified by the datasheet for stability. Additionally, each power rail has distributed decoupling capacitors across the **PCB** placed near loads. The decoupling strategy uses three capacitor values in parallel at each location: 10 microfarads, 100 nanofarads, and 10 nanofarads. This combination provides low impedance across the frequency range from DC to 100 MHz.

The 10 microfarad capacitors (X7R ceramic or tantalum) store enough charge to supply transient current demands without significant voltage droop. When the microcontroller suddenly increases current draw during processing bursts, these capacitors provide the charge before the LDO can respond. The droop is:

$$
\Delta V = \frac{Q}{C} = \frac{I \Delta t}{C}
$$

For a 100 milliamp load step lasting 1 microsecond with 10 microfarad capacitance:

$$
\Delta V = \frac{0.1 \times 10^{-6}}{10 \times 10^{-6}} = 10 \text{ mV}
$$

This 10 millivolt transient is acceptable and is further suppressed by the 100 nanofarad and 10 nanofarad capacitors that handle faster transients.

**[SVG DIAGRAM 12: Power supply schematic showing the USB 5V input, three LDO regulators (TPS7A4700 for 3.3V, two TPS7A33 for 1.8V rails), and their associated input/output capacitors. Include current consumption annotations for each load]**

### Thermal Management

Power dissipation in the system is modest, approximately 2 watts total under maximum processing load. This power is distributed across several components rather than concentrated in a single hotspot. The primary heat sources are:

STM32H753: Approximately 1.2 watts at 480 MHz full operation. The LQFP144 package has thermal resistance of approximately 38 °C/W junction-to-ambient with no heatsink. With 1.2 watts dissipation and 25 °C ambient:

$$
T_{\text{junction}} = T_{\text{ambient}} + P \times R_{\theta JA} = 25 + 1.2 \times 38 = 70.6 \text{ °C}
$$

This junction temperature remains well below the 85 °C maximum, providing adequate margin.

LDO regulators: Each regulator dissipates power equal to the current through it multiplied by the voltage drop across it. For the 3.3 volt regulator supplying 200 milliamps from 5 volts:

$$
P = I(V_{\text{in}} - V_{\text{out}}) = 0.2 \times (5 - 3.3) = 0.34 \text{ watts}
$$

The SOT-23 package typical of small LDOs has thermal resistance of approximately 100 °C/W, yielding temperature rise of 34 °C. We specify regulators in larger packages or with thermal pads connected to **PCB** copper pours to improve heat dissipation.

The microphones dissipate only 1.2 milliwatts each, producing negligible self-heating.

No active cooling is required. The **PCB** acts as a heat spreader, conducting heat from the components to the ambient air through convection. Copper pours on the inner layers improve thermal conductivity. In an enclosed system, we would ensure adequate ventilation, but the low power dissipation permits fairly restrictive enclosures without overheating.

## Digital Signal Processing Algorithms

Real-time processing of the six-channel audio stream enables extraction of spatial information and detection of events of interest. The algorithms implemented must operate within the computational capacity of the STM32H753 while maintaining sub-20 millisecond latency. This section details the processing pipeline and provides computational cost estimates.

### Delay-and-Sum Beamforming

Beamforming is the fundamental spatial filtering operation that increases sensitivity to sounds arriving from a specified direction while attenuating sounds from other directions. The simplest form is delay-and-sum beamforming, which applies time delays to align the signals from different microphones and then sums them.

For a linear array of $N$ microphones receiving a plane wave from angle $\theta$ relative to the array normal, the acoustic delay to microphone $n$ relative to a reference point is:

$$
\tau_n = \frac{n d \sin(\theta)}{v_{\text{sound}}}
$$

To steer the beam to angle $\theta_0$, we apply compensating delays $-\tau_n(\theta_0)$ to each microphone signal. After delay compensation, signals from angle $\theta_0$ align in phase and sum coherently, while signals from other angles suffer destructive interference. The beam former output is:

$$
y(t) = \frac{1}{N} \sum_{n=0}^{N-1} x_n(t - \tau_n(\theta_0))
$$

Implementing fractional-sample delays in discrete-time systems requires interpolation. For delays that are not integer multiples of the sample period, we use either polynomial interpolation or FIR fractional delay filters. A simple approach is linear interpolation:

$$
x(t - \tau) \approx (1 - \alpha) x[k] + \alpha x[k-1]
$$

where $k$ is the integer part of $\tau f_s$ and $\alpha$ is the fractional part. This first-order interpolation introduces some high-frequency attenuation but provides sufficient accuracy for beamforming applications where we are primarily interested in mid-band frequencies.

For our vertical arrays with 15 millimeter spacing and 48 kHz sample rate, the maximum delay between top and bottom microphones for 90° steering is:

$$
\tau_{\text{max}} = \frac{2 \times 0.015}{343} = 87.5 \text{ μs} = 4.2 \text{ samples}
$$

The computational cost of delay-and-sum beamforming for one array (three microphones) is approximately:

3 multiplications per sample (for fractional delays)
3 additions per sample (for summation)
Total: 6 operations per sample per beam

At 48 kHz with two beams (left and right blocks), this requires:

$$
\text{Operations/s} = 2 \times 6 \times 48000 = 576000 \text{ ops/s} \approx 0.6 \text{ MIPS}
$$

This negligible cost allows continuous real-time beamforming on both arrays simultaneously.

**[SVG DIAGRAM 13: Conceptual illustration of delay-and-sum beamforming on a three-element vertical array. Show plane waves arriving at angle θ, the geometric delays, the compensating delays applied digitally, and the coherent summation. Include signal waveforms before and after alignment]**

### Direction of Arrival Estimation via GCC-PHAT

Determining the direction from which a sound originates is accomplished through the Generalized Cross-Correlation with Phase Transform (GCC-PHAT) algorithm. This method estimates the time delay between two signals and is robust to reverberation and frequency-dependent attenuation.

For two signals $x_1(t)$ and $x_2(t)$ captured by the left and right microphone blocks, we first compute the cross-power spectral density in the frequency domain:

$$
G_{12}(f) = X_1(f) X_2^*(f)
$$

where $X_1(f)$ and $X_2(f)$ are obtained via Fast Fourier Transform and $*$ denotes complex conjugate. The GCC-PHAT applies phase-transform weighting:

$$
\Psi_{\text{PHAT}}(f) = \frac{1}{|G_{12}(f)|}
$$

This normalization emphasizes phase information while de-emphasizing magnitude, making the method robust to spectral coloration and echo returns. The weighted cross-spectrum becomes:

$$
R_{\text{PHAT}}(f) = \Psi_{\text{PHAT}}(f) G_{12}(f) = \frac{G_{12}(f)}{|G_{12}(f)|} = e^{j\phi(f)}
$$

where $\phi(f)$ is the phase angle of the cross-spectrum. Taking the inverse FFT yields the generalized cross-correlation function:

$$
r_{\text{PHAT}}(\tau) = \mathcal{F}^{-1}\{R_{\text{PHAT}}(f)\}
$$

The peak of this function indicates the time delay between the signals:

$$
\hat{\tau} = \arg\max_{\tau} |r_{\text{PHAT}}(\tau)|
$$

Finally, the azimuth angle is calculated from the estimated delay using the inverse of the **ITD** relationship:

$$
\hat{\phi} = \arcsin\left(\frac{v_{\text{sound}} \times \hat{\tau}}{d_{\text{separation}}}\right)
$$

The GCC-PHAT is computed periodically rather than on every sample. We use frames of 512 samples (10.67 milliseconds at 48 kHz), zero-padded to 1024 points for FFT efficiency. The computational cost per frame includes:

Two 1024-point FFTs (forward transforms of the two signals): $2 \times 1024 \log_2(1024) = 2 \times 10240 = 20480$ multiplications
Complex multiplication for cross-power spectrum: 1024 complex products = 4096 real multiplications
Normalization by magnitude: 1024 divisions (equivalent cost to 4096 multiplications on modern CPUs)
One 1024-point IFFT: 10240 multiplications
Peak search: 1024 comparisons (negligible cost)

Total: approximately 39000 multiplications per frame, executed once every 10.67 milliseconds, yielding average cost of:

$$
\text{MIPS}_{\text{GCC-PHAT}} = \frac{39000}{0.01067 \times 10^6} \approx 3.7 \text{ MIPS}
$$

This is well within the capability of the STM32H753, especially considering that the Cortex-M7 includes a hardware floating-point unit that accelerates these operations.

**[SVG DIAGRAM 14: Graph showing a typical GCC-PHAT function with a sharp peak at the time delay corresponding to the source direction. X-axis shows time delay in microseconds from -500 to +500, y-axis shows correlation magnitude normalized to 0-1. Include annotation pointing to the peak with the calculated angle]**

### Computational Budget Analysis

To verify that the STM32H753 can execute all required processing in real-time, we construct a detailed computational budget. Operating at 48 kHz sample rate with 512-sample frames (10.67 milliseconds), we have:

$$
\text{Available cycles per frame} = 480 \times 10^6 \text{ Hz} \times 0.01067 \text{ s} = 5.12 \times 10^6 \text{ cycles}
$$

Assuming average efficiency of 0.7 MIPS per MHz (accounting for memory access overhead and instruction mix), effective compute capacity is:

$$
\text{Effective operations} = 0.7 \times 5.12 \times 10^6 = 3.58 \times 10^6 \text{ ops}
$$

The processing tasks and their approximate costs are:

DFSDM output processing (six channels): Each sample requires copying from peripheral to RAM and possibly format conversion. Cost: approximately 6 cycles per sample × 6 channels × 512 samples = 18432 cycles

Delay-and-sum beamforming (two beams): 6 operations per sample × 512 samples × 2 beams = 6144 operations

GCC-PHAT direction estimation: 39000 operations (calculated previously)

Event detection: Spectral analysis via 512-point FFT (2560 operations) plus threshold comparisons (negligible). Total: approximately 3000 operations

USB buffer management: Copying samples to USB transfer buffers and triggering transfers. Cost: approximately 10000 cycles

Overhead (interrupt latency, context switching, operating system if used): Approximately 10% of total = 360000 cycles

Total: 18432 + 6144 + 39000 + 3000 + 10000 + 360000 = 436576 operations

This represents only 12.2% of the available 3.58 million operations per frame, leaving enormous margin for additional processing such as adaptive beamforming, noise suppression, or more sophisticated event detection algorithms. The design is intentionally over-provisioned to ensure reliable real-time operation even under worst-case conditions.

**[SVG DIAGRAM 15: Computational budget pie chart showing the percentage allocation of processing time: DFSDM processing (5%), Beamforming (2%), GCC-PHAT (11%), Event Detection (1%), USB Management (3%), Overhead (10%), and Unused Margin (68%)]**

### Latency Analysis

System latency is the time from acoustic event to digital output availability at the USB interface. Understanding latency is critical for applications requiring real-time response or synchronization with other sensors. We can decompose total latency into contributions from each stage:

**Microphone acoustic-to-digital latency**: **MEMS** microphones exhibit internal delays from the time pressure variations reach the diaphragm until the **PDM** bitstream reflects the change. Typical latency is 0.5 to 1.0 milliseconds. We estimate 0.75 milliseconds for the IM69D130.

**DFSDM filtering latency**: The sinc filter implements a moving average over multiple **PDM** samples. For our third-order sinc filter with decimation factor 50, the group delay is:

$$
\tau_{\text{DFSDM}} = \frac{3 \times 50}{2 \times 2.4 \times 10^6} = 31.25 \text{ μs}
$$

This is negligible compared to other latencies.

**DMA and buffering latency**: Samples are transferred from DFSDM to RAM via DMA and accumulate in buffers until a complete frame of 512 samples is available for processing. Average latency is half the frame duration:

$$
\tau_{\text{buffer}} = \frac{512}{2 \times 48000} = 5.33 \text{ ms}
$$

**Processing latency**: Computing beamforming and GCC-PHAT on 512-sample frames requires approximately 0.5 milliseconds based on our computational budget (436576 operations at 480 MHz effective performance of 0.7 × 480 MIPS = 336 MIPS implies execution time of 436576/336000000 = 1.3 milliseconds worst-case, typically faster).

**USB transfer latency**: Isochronous transfers occur every 1 millisecond (USB frame period). Data must wait for the next transfer opportunity, adding average latency of 0.5 milliseconds. Additionally, the USB stack implements buffering to absorb timing variations. The TinyUSB implementation typically buffers 2 to 5 milliseconds. We estimate 3 milliseconds total USB latency conservatively.

**Total latency**:

$$
\tau_{\text{total}} = 0.75 + 0.03 + 5.33 + 1.3 + 3.0 = 10.4 \text{ ms}
$$

This analysis yields approximately 10 milliseconds latency, well below our 20 millisecond target. In practice, latency measurements on similar systems confirm values in the 8 to 15 millisecond range depending on host computer USB timing characteristics.

For applications requiring lower latency, we can reduce the processing frame size from 512 to 256 or 128 samples, trading increased computational overhead for reduced buffering delay. At 128 samples, latency would decrease to approximately 5 milliseconds, approaching the limits imposed by the inherent delays in the microphones and USB transport.

## Performance Analysis and Capabilities

Having detailed the architecture and algorithms, we now quantify the expected performance of the complete system across several metrics relevant to the intended applications. These predictions are based on established theory, component specifications, and comparison with similar implemented systems.

### Dynamic Range

The total system dynamic range is limited by the weakest element in the signal chain. For our all-digital architecture, this is the microphone, as the digital path introduces no additional noise or distortion beyond quantization effects that are negligible at 24-bit resolution.

The IM69D130 specifies **SNR** of 69 dB (A-weighted), meaning that the microphone self-noise is 69 dB below a 1 kHz tone at 94 dB **SPL**. The acoustic noise floor is therefore:

$$
\text{Noise floor} = 94 - 69 = 25 \text{ dB **SPL**}
$$

This represents the minimum detectable sound level in ideal conditions (anechoic environment with no wind or electronic interference). For comparison, a typical quiet room exhibits ambient noise of 30 to 40 dB **SPL**, so the microphone noise floor is rarely the limiting factor in practice.

The Acoustic Overload Point (AOP) is specified as 130 dB **SPL** for 10% THD. This is the maximum sound level the microphone can capture before severe distortion occurs. The useful dynamic range is therefore:

$$
\text{Dynamic range} = \text{AOP} - \text{Noise floor} = 130 - 25 = 105 \text{ dB}
$$

This 105 dB dynamic range captures sounds from very quiet (whispers at 30 dB **SPL**) to very loud (jet engines at 130 dB **SPL**) without clipping or excessive noise. The system meets our target of 120 dB dynamic range with the understanding that the upper 10 dB (130 to 140 dB **SPL**) would require microphones with higher AOP, which are not available in **MEMS** technology at reasonable cost.

For context, typical sound levels relevant to our applications include:

Normal conversation: 60 dB **SPL**
Traffic noise: 80 dB **SPL**
Explosion at 100 meters: 110 to 120 dB **SPL**
Aircraft at 100 meters: 120 to 130 dB **SPL**

All of these fall well within our 105 dB captured range.

### Frequency Response

The system frequency response is determined primarily by the microphones. The IM69D130 specifies:

$$
\pm 1 \text{ dB from 20 Hz to 10 kHz}
$$
$$
\pm 3 \text{ dB from 20 Hz to 20 kHz}
$$

This specification indicates flat response across the critical mid-band where human speech and most acoustic events concentrate their energy, with gradual roll-off at the extreme high frequencies. The ±3 dB tolerance at 20 kHz means that some units may exhibit response as low as -3 dB while others are as high as +3 dB, a 6 dB spread. This variability is typical of **MEMS** microphones and can be calibrated out if necessary by measuring each unit and applying equalization.

The DFSDM filtering introduces additional high-frequency attenuation beyond the microphone response. The sinc filter has magnitude response:

$$
|H(f)| = \left|\frac{\sin(\pi f / f_s)}{\pi f / f_s}\right|
$$

At 20 kHz with 48 kHz sample rate, this evaluates to:

$$
|H(20 \text{ kHz})| = \left|\frac{\sin(\pi \times 20000/48000)}{\pi \times 20000/48000}\right| = 0.866 = -1.2 \text{ dB}
$$

Combined with the microphone response, the total system response at 20 kHz ranges from -4.2 to -1.2 dB, depending on microphone unit variation. This is acceptable for high-quality audio capture where perfect flatness to 20 kHz is rarely achieved even in expensive systems.

Below 10 kHz, the system response is essentially flat within ±1 dB, providing accurate capture of all acoustically relevant content for speech, music, and environmental sounds.

### Spatial Resolution

Spatial resolution in azimuth can be predicted from the Cramér-Rao Lower Bound (CRLB), which establishes the theoretical minimum variance achievable by any unbiased estimator. For time delay estimation between two microphones, the CRLB is:

$$
\text{Var}(\hat{\tau}) \geq \frac{1}{8\pi^2 \beta^2 \text{SNR}}
$$

where $\beta$ is the RMS bandwidth of the signal and **SNR** is the signal-to-noise ratio. For a broadband signal with bandwidth from 100 Hz to 4 kHz (typical speech), the RMS bandwidth is approximately:

$$
\beta = \sqrt{\frac{f_{\text{high}}^2 + f_{\text{low}}^2}{2}} \approx 2.8 \text{ kHz}
$$

With **SNR** of 20 dB (typical in noisy environments):

$$
\sigma_{\tau} = \sqrt{\text{Var}(\hat{\tau})} = \frac{1}{2\pi \beta \sqrt{2 \text{SNR}}} = \frac{1}{2\pi \times 2800 \times \sqrt{200}} \approx 4 \text{ μs}
$$

Converting time delay standard deviation to angle using our 100 millimeter baseline:

$$
\sigma_{\phi} = \arcsin\left(\frac{v_{\text{sound}} \sigma_{\tau}}{d}\right) \times \frac{180}{\pi} = \arcsin\left(\frac{343 \times 4 \times 10^{-6}}{0.1}\right) \times \frac{180}{\pi} \approx 0.78°
$$

This CRLB analysis predicts sub-degree resolution under ideal conditions. However, real-world factors degrade performance. Reverberation adds delayed copies of the signal that interfere with delay estimation. Competing sound sources create situations where the GCC-PHAT may lock onto the wrong source. Finite data record length (our 512-sample frames) limits the precision of frequency estimation.

Accounting for these factors, empirical testing of similar systems achieves angular resolution of 3 to 5 degrees in typical indoor environments and 2 to 3 degrees in anechoic conditions. This matches our design target and represents state-of-the-art performance for arrays of this size.

In elevation, the vertical arrays provide similar resolution limited by the narrower aperture (30 millimeters vs 100 millimeters). We expect elevation resolution of 5 to 10 degrees, which is sufficient to distinguish between sources at different heights but not sufficient for precision elevation tracking.

**[SVG DIAGRAM 16: Polar plot showing the expected angular resolution in azimuth. Plot the probability density function of angle estimates for a true source at 45 degrees, showing the 3-5 degree spread. Include comparison to human angular resolution for context]**

### Beamforming Gain

Beamforming provides gain against spatially uncorrelated noise while maintaining unity gain for signals arriving from the steering direction. The theoretical gain for a uniform array is:

$$
G_{\text{array}} = 10 \log_{10}(N)
$$

For our three-element vertical arrays, this predicts 4.8 dB gain. However, this assumes perfect coherence of the desired signal across all elements and complete incoherence of the noise. In practice, the desired signal has finite coherence due to scattering and diffraction, and the noise has some spatial correlation due to common sources.

A more realistic model accounts for these effects using the coherence factor $\gamma$, which ranges from 0 (no coherence) to 1 (perfect coherence):

$$
G_{\text{practical}} = 10 \log_{10}(N \gamma)
$$

For typical indoor environments at mid frequencies (1 to 4 kHz) with our array geometry, $\gamma \approx 0.7$ to 0.8, yielding practical gains of:

$$
G_{\text{practical}} = 10 \log_{10}(3 \times 0.75) = 3.5 \text{ dB}
$$

This 3.5 dB gain provides noticeable improvement in signal-to-noise ratio, equivalent to moving the desired source 1.5 times closer or reducing the background noise by a factor of 2.2. For applications like speech enhancement or selective listening, this gain can make the difference between intelligible and unintelligible communication.

The binaural processing using both left and right blocks provides additional gain through spatial filtering that rejects sources outside the target azimuth. The combined system (vertical beamforming plus binaural processing) can achieve overall gains of 8 to 12 dB for well-isolated sources, though this requires adaptive processing beyond the basic delay-and-sum approach.

## Operating Modes and Applications

The firmware implements multiple operating modes to serve different application requirements. Each mode makes different trade-offs between data rate, processing complexity, and information content. Users can select modes via USB control commands or configuration files.

### Mode 1: Raw Streaming

In raw streaming mode, all six microphone channels are transmitted to the host computer at 48 kHz sample rate with 24-bit resolution. No processing occurs beyond the DFSDM filtering inherent in PDM-to-PCM conversion. This mode serves several purposes:

**Hardware validation**: During system bring-up, raw streaming allows verification that all six microphones are functioning and properly synchronized. Captured data can be analyzed offline to measure channel matching, frequency response, and noise floors.

**Algorithm development**: Researchers developing new beamforming or localization algorithms can capture raw multi-channel data for offline processing and evaluation. This eliminates the need to implement algorithms on the embedded platform during the development phase.

**Forensic recording**: Applications requiring preservation of complete spatial information for later analysis benefit from raw capture. The full six-channel recording contains the maximum information content, enabling post-processing with algorithms not yet developed.

The data rate for raw streaming is:

$$
\text{Data rate} = 6 \text{ channels} \times 3 \text{ bytes/sample} \times 48000 \text{ Hz} = 864 \text{ kB/s}
$$

This rate is easily sustained by USB High-Speed and requires modest host computer processing. A typical laptop can record many hours of six-channel audio without difficulty.

### Mode 2: Processed Binaural

Processed binaural mode implements on-board beamforming to reduce the six channels to two (left and right) while extracting spatial information. The left beam is steered to optimize reception from the left hemisphere, and likewise for the right beam. The result is a stereo output that preserves spatial cues while reducing data rate.

In addition to the two audio channels, processed binaural mode transmits metadata including:

**Estimated azimuth angle**: Updated every 10.67 milliseconds based on GCC-PHAT analysis, providing 93 updates per second. The angle is quantized to 0.1 degree resolution and transmitted as a 12-bit value.

**Confidence metric**: A measure of how peaked the GCC-PHAT correlation function is, indicating reliability of the angle estimate. Values range from 0 (completely ambiguous) to 100 (very confident).

**RMS level per channel**: Integrated over each frame, providing information about signal strength useful for adaptive processing or voice activity detection.

The data rate for processed binaural mode is:

$$
\text{Audio rate} = 2 \text{ channels} \times 3 \text{ bytes/sample} \times 48000 \text{ Hz} = 288 \text{ kB/s}
$$

Plus metadata at approximately 12 kB/s, yielding total data rate of 300 kB/s, reduced by a factor of 2.9 compared to raw streaming. This mode is recommended for continuous operation with recording, as it provides both high-quality audio and spatial information at sustainable data rates.

### Mode 3: Event Detection

Event detection mode operates continuously but transmits data only when acoustic events of interest are detected. The system maintains a circular buffer of 2 seconds duration. When an event is detected, the buffer contents are transmitted along with metadata describing the event type and characteristics.

Event detectors implemented include:

**Explosion detector**: Searches for sudden level increases exceeding 20 dB within 50 milliseconds, combined with spectral content concentrated below 200 Hz. This signature matches impulsive events like gunshots, fireworks, or actual explosions. False positives occur with door slams and loud percussive sounds, but these are generally acceptable as they represent events of interest.

**Aircraft detector**: Identifies sustained broadband noise with fundamental frequency in the range 50 to 500 Hz and duration exceeding 1 second. The detector tracks frequency shifts (Doppler) to distinguish moving aircraft from stationary machinery. Harmonic structure helps reject pure-tone noises from electronic devices.

**Loud event detector**: Simple threshold detector triggering on any sound exceeding 90 dB **SPL**, useful for general activity monitoring where any loud event is relevant.

When an event is detected, the system transmits:

2 seconds of pre-event audio (from the circular buffer) at two-channel beamformed format
Event audio (duration varies, typically 5 to 10 seconds) at two-channel format
Event metadata (type, start time, level, spectral characteristics)

Average data rate depends on event frequency. For environments with one event per minute, average rate is:

$$
\text{Average rate} = \frac{10 \text{ s}}{60 \text{ s}} \times 288 \text{ kB/s} \approx 48 \text{ kB/s}
$$

This order-of-magnitude reduction in data rate enables long-term unattended recording on systems with limited storage.

### Mode 4: Direction Monitoring

Direction monitoring mode provides continuous azimuth angle estimates at high update rate (93 Hz) with minimal audio. Only short audio clips (100 milliseconds) are transmitted when the estimated direction changes by more than a threshold (typically 10 degrees), providing context for what was heard in that direction.

This mode serves applications where spatial tracking is more important than complete audio recording, such as:

**Wildlife monitoring**: Tracking animal vocalizations without storing hours of silence
**Security**: Detecting directional cues to unusual activity without continuously recording private conversations
**Source counting**: Estimating the number of simultaneous sources based on direction changes

Data rate in direction monitoring mode averages 5 to 10 kB/s, enabling months of operation on modest storage.

**[SVG DIAGRAM 17: Four panel diagram showing waveforms and data outputs for each operating mode. Panel 1 shows six-channel raw waveforms. Panel 2 shows two-channel beamformed output plus angle track. Panel 3 shows event-triggered recording with pre/post buffers. Panel 4 shows sparse angle updates with audio snippets]**

## Bill of Materials and Cost Analysis

Detailed cost estimation is essential for evaluating project feasibility and planning for potential production. The following analysis uses component pricing from major distributors (Digi-Key, Mouser) for single-unit quantities typical of prototyping. Prices would decrease substantially in production volumes.

### Component Costs

| Category | Component | Quantity | Unit Cost | Extended |
|----------|-----------|----------|-----------|----------|
| **Microphones** | Infineon IM69D130 | 6 | $2.50 | $15.00 |
| **Microcontroller** | STMicro STM32H753ZIT6 | 1 | $12.00 | $12.00 |
| **Voltage Regulators** | TI TPS7A4700 (3.3V) | 1 | $3.00 | $3.00 |
| | TI TPS7A33 (1.8V mic) | 1 | $2.80 | $2.80 |
| | TI TPS7A33 (1.8V analog) | 1 | $2.80 | $2.80 |
| **Oscillator** | 25 MHz crystal | 1 | $0.50 | $0.50 |
| **USB Interface** | USB Type-C connector | 1 | $1.00 | $1.00 |
| **Passives** | Capacitors (>80 total) | lot | -- | $12.00 |
| | Resistors (>40 total) | lot | -- | $3.00 |
| | Ferrite beads | 5 | $0.20 | $1.00 |
| **PCB** | 4-layer, 110×50mm | 1 | -- | $18.00 |
| **Miscellaneous** | Headers, test points | lot | -- | $3.00 |
| **TOTAL** | | | | **$74.10** |

This $74 total represents a conservative estimate for prototype quantities. Several opportunities exist for cost reduction:

**Volume pricing**: Purchasing 100 units rather than 1 reduces component costs by approximately 30 to 50%. The microcontroller price drops to approximately $8, microphones to $1.80 each, and regulators to $2 each. Total component cost would fall to approximately $55 in moderate volumes.

**PCB costs**: Four-layer **PCB** pricing decreases dramatically with panel efficiency. Ordering 10 copies on a shared panel reduces per-board cost to approximately $8. For 100 units, per-board cost approaches $3.

**Assembly**: The BOM analysis above assumes hand assembly. Automated pick-and-place assembly adds approximately $0.50 per board at 100-unit volumes, but this is offset by reduced labor costs and improved reliability.

### Comparison to Alternatives

To contextualize the cost, we compare to existing commercial products with similar capabilities:

**ReSpeaker Mic Array v2.0**: Four-microphone array with AC108 codec, approximately $50. Our design offers superior spatial resolution (six microphones in optimized geometry vs four in square), better microphone performance (69 dB **SNR** vs 63 dB SNR), and more powerful processing (480 MHz vs 400 MHz), justifying the modest cost increase.

**miniDSP UMA-8**: Eight-channel USB microphone array with professional specifications, approximately $200. This system targets professional audio markets with very high quality microphones and sophisticated processing, explaining the higher cost. Our design occupies the middle ground between the ReSpeaker (hobbyist) and UMA-8 (professional).

**Generic USB studio microphone**: A single high-quality condenser microphone suitable for recording costs $100 to $300. Our six-microphone array at $74 represents exceptional value for applications requiring spatial information that no single microphone can provide.

## Conclusions and Future Development

The proposed superhuman hearing system represents a careful balance of performance, complexity, and cost. By adopting an all-digital microphone architecture with six **MEMS** devices in a binaural array configuration, we achieve several design objectives simultaneously.

### Achieved Capabilities

The system provides spatial localization with 3 to 5 degree resolution in azimuth and 5 to 10 degrees in elevation, approaching or exceeding human performance in the frontal hemisphere. The 100 millimeter baseline between left and right microphone blocks enables exploitation of both interaural time difference and level difference cues across the full audible spectrum.

Frequency response covers 20 Hz to 20 kHz with flatness within ±1 dB from 20 Hz to 10 kHz, capturing all acoustically relevant content for speech, music, and environmental sounds. The 105 dB dynamic range spans from quiet rooms at 30 dB **SPL** to loud machinery at 130 dB **SPL** without clipping or excessive noise.

Real-time processing on the STM32H753 implements beamforming, direction-of-arrival estimation, and event detection with total latency below 11 milliseconds. The 68% unused computational margin provides substantial headroom for future algorithm enhancements without hardware changes.

The USB Audio Class 2.0 interface ensures plug-and-play operation on all modern operating systems without proprietary drivers. Multiple operating modes (raw streaming, processed binaural, event detection, direction monitoring) adapt the system to diverse applications from forensic recording to wildlife tracking.

Total bill-of-materials cost of approximately $74 for prototypes, dropping to $55 in moderate production volumes, makes the system accessible for research, education, and commercial product development.

### Advantages of the Digital Architecture

The decision to adopt digital **MEMS** microphones rather than analog microphones with an external codec proved highly beneficial. Simplified **PCB** design eliminates analog signal routing challenges and reduces sensitivity to electromagnetic interference. Component count decreased by removing the codec, associated power supply filtering, and clock distribution complexity. Guaranteed synchronization between channels comes from shared **PDM** clock rather than careful matching of multiple ADC clocks.

The all-digital approach enables future improvements through firmware updates rather than hardware revisions. Adaptive beamforming algorithms, more sophisticated event detectors, and machine learning classifiers can be developed and deployed without changing the hardware platform.

### Limitations and Trade-offs

The system does not extend into the ultrasonic region above 20 kHz. This was a conscious trade-off accepting that the vast majority of acoustic events of interest concentrate their informative content below 20 kHz. Applications specifically requiring ultrasonic detection (bat monitoring, ultrasonic leak detection) would need either different microphones or an auxiliary ultrasonic module.

Spatial resolution degrades at high frequencies above 11.4 kHz due to spatial aliasing resulting from the 15 millimeter microphone spacing. Reducing spacing to 8 millimeters would eliminate aliasing but complicate **PCB** assembly and reduce vertical aperture. The chosen spacing optimizes performance in the critical 1 to 10 kHz range where most spatial localization occurs.

The compact array size (100 × 30 millimeters) provides limited low-frequency directivity. Below approximately 500 Hz, the array dimensions become small compared to wavelength, and spatial resolution degrades. Large arrays with multi-meter baselines would be required for precise localization of low-frequency sources like distant explosions or heavy machinery.

### Next Steps for Implementation

The immediate next step is detailed **PCB** design following the guidelines established in this document. Simulation of the **PDM** clock distribution tree and USB differential pair should verify signal integrity before board fabrication. Thermal simulation confirms that the identified heat dissipation approaches maintain all components within operating temperature ranges.

Firmware development begins with peripheral drivers (DFSDM, DMA, USB) using the STM32CubeMX code generation tool and the TinyUSB stack. Basic functionality testing verifies that all six microphones produce sensible outputs and maintain synchronization. Once the hardware platform is validated, algorithm implementation proceeds incrementally: delay-and-sum beamforming first, then GCC-PHAT direction estimation, and finally event detection.

Acoustic characterization in an anechoic chamber measures frequency response, phase matching between channels, and angular resolution. Known source positions at various angles verify the accuracy of the direction estimation algorithm. Measurements of performance in reverberant environments assess robustness to echoes.

Field testing in representative application environments (urban monitoring for security applications, forest setting for wildlife applications) identifies practical limitations and guides refinement of processing algorithms. User feedback during alpha testing informs final adjustments before declaring the platform ready for broader deployment.

### Potential Enhancements

Several enhancements could be pursued depending on specific application requirements and available development resources. Adaptive beamforming using the MVDR (Minimum Variance Distortionless Response) algorithm would improve interference rejection compared to fixed delay-and-sum. This requires estimating the spatial correlation matrix of the noise, which can be done during voice activity gaps or using robust estimation methods.

Machine learning classification of acoustic events would provide more reliable event detection than hand-crafted threshold detectors. A small neural network running on the Cortex-M7 could classify sounds into categories (speech, music, vehicle, animal, machinery) trained on labeled data. The STM32H753 is capable of real-time neural network inference for compact models with careful optimization.

Extending dynamic range beyond the 105 dB provided by the microphones is possible using multi-capsule techniques. Two microphones at each array position with different gains (0 dB and +20 dB) effectively extend dynamic range to 125 dB by selecting the appropriate capsule based on signal level. This doubles microphone count and cost but could be valuable for applications requiring capture of very weak sounds in loud environments.

Integration with other sensor modalities (camera, IMU, GPS) would enable multi-modal perception systems. Fusing acoustic localization with visual detection improves reliability and enables tracking of sources that move between the audio and visual fields of view. An inertial measurement unit compensates for platform motion when the array is carried by a mobile robot or worn by a person.

This project demonstrates that sophisticated spatial audio systems can be implemented with modest cost and complexity using modern digital components and signal processing. The capabilities achieved rival or exceed biological auditory systems in specific aspects while maintaining the flexibility and upgradability inherent to digital systems. As **MEMS** microphone technology continues advancing and microcontroller performance improves, even more capable systems will become accessible to researchers and developers.


## First Detailed Bill of Materials

| Category              | Component / Part Number          | Quantity | Description / Notes                                                                 |
|-----------------------|----------------------------------|----------|-------------------------------------------------------------------------------------|
| **Microcontroller**   | **STM32H753ZIT6**  | 1        | Cortex-M7 480 MHz.               |
| **Microphones**       | **Infineon IM69D130**            | 6        | Digital MEMS PDM, 69dB SNR. OK.                                                     |
| **Voltage Regulator** | **TI TPS7A4700**                 | 1        | 5V to 3.3V low-noise. OK.                                                          |
| **Voltage Regulator** | **TI TPS7A33**                   | 2        | 3.3V to 1.8V (Mics + Analog). Corrected from 3301.                                 |
| **Clock/Timing**      | **25 MHz Crystal**               | 1        | ±20ppm. OK.                                                                         |
| **Connector**         | **2x20 Pin GPIO Header**         | 1        | Pass-through. OK.                                                                   |
| **Connector**         | **USB-C Receptacle**             | 1        | For USB Audio output. Added.                                                        |
| **Passive (Caps)**    | **Ceramic Capacitor 10µF**       | ~15      | Bulk decoupling. OK.                                                                |
| **Passive (Caps)**    | **Ceramic Capacitor 100nF**      | ~30      | Local decoupling. OK.                                                               |
| **Passive (Caps)**    | **Ceramic Capacitor 10nF**       | ~30      | HF filtering. OK.                                                                   |
| **Passive (Caps)**    | **Load Capacitors (18pF)**       | 2        | For crystal. OK.                                                                    |
| **Passive (Res)**     | **Resistor 10kΩ**                | ~10      | Pull-ups. OK.                                                                       |
| **Passive (Res)**     | **Resistor 33Ω**                 | ~12      | Series for PDM/USB. OK.                                                             |
| **Passive (Res)**     | **Resistor 0Ω**                  | ~5       | Jumpers. OK.                                                                        |
| **Passive (Ind)**     | **Ferrite Bead**                 | 5        | EMI suppression. OK.                                                                |
| **Protection**        | **TVS Diode (ex: ESD5V3U1U)**    | ~5       | ESD for GPIO/USB. Added.                                                            |
| **HAT ID**            | **Microchip 24LC32A**            | 1        | EEPROM for HAT detection. Added.                                                    |
| **Debug**             | **Tactile Button + LED**         | 2        | Reset/Status. Added.                                                                |
| **PCB**               | **Custom 4-Layer FR-4**          | 1        | 110x50mm, with acoustic holes. Adjust size if for Pi standard.                      |
| **Mechanical**        | **M2.5 Standoffs & Screws**      | 4        | HAT mounting. OK.                                                                   |